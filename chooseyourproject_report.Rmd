---
title: "Animal Shelter Outcome Prediction"
author: "Yvonne Kirschler"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    number_sections: true
    toc_depth: 2
  html_document:
    toc: true
    number_sections: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(caret)
library(randomForest)
library(scales)
library(tidytext)  # for reorder_within and scale_x_reordered
```

# Introduction

This report is part of the **HarvardX Data Science Capstone â€“ Choose Your Own** module.  
It explores the challenge of predicting animal outcomes (e.g. Adoption, Transfer, Euthanasia)  
at the **Austin Animal Center**, using available intake data.

The dataset contains over 79,000 records and includes features such as:

- animal type  
- intake type  
- intake condition  
- sex upon intake

**Goal:** Predict the `outcome_type` using machine learning  
**Main challenge:** High class imbalance (e.g., ~42% Adoption)

# Methods

## Data loading

```{r}
data <- read_csv("data/archive/aac_intakes_outcomes.csv")
glimpse(data)
```

## Exploratory data analysis

```{r}
# Outcome distribution
data %>% count(outcome_type) %>%
  ggplot(aes(x = fct_infreq(outcome_type), y = n)) +
  geom_col(fill = "steelblue") +
  labs(title = "Outcome Types", x = "Outcome Type", y = "Count") +
  theme_minimal()

# Intake type distribution
data %>% count(intake_type) %>%
  ggplot(aes(x = fct_infreq(intake_type), y = n)) +
  geom_col(fill = "darkgreen") +
  labs(title = "Intake Type", x = "Intake Type", y = "Count") +
  theme_minimal()

# Sex upon intake
data %>% count(sex_upon_intake) %>%
  ggplot(aes(x = fct_infreq(sex_upon_intake), y = n)) +
  geom_col(fill = "darkorange") +
  labs(title = "Sex upon Intake", x = "Sex", y = "Count") +
  theme_minimal()
```

## Data preprocessing

Only variables that are known at the time of intake were selected,  
to simulate a real-time decision support scenario.

```{r}
data <- data %>%
  filter(!is.na(outcome_type)) %>%
  drop_na(animal_type, intake_type, sex_upon_intake, intake_condition)
```

## Train/test split

```{r}
set.seed(42)
train_index <- createDataPartition(data$outcome_type, p = 0.8, list = FALSE)
train_set <- data[train_index, ]
test_set  <- data[-train_index, ]
```

# Modeling

## Baseline model

```{r}
most_common <- train_set %>%
  count(outcome_type) %>%
  slice_max(n, n = 1) %>%
  pull(outcome_type)

baseline_predictions <- rep(most_common, nrow(test_set))
baseline_accuracy <- mean(baseline_predictions == test_set$outcome_type)
baseline_accuracy
```

## Random Forest model (5-fold CV)

```{r}
rf_data <- train_set %>%
  mutate(outcome_type = as.factor(outcome_type)) %>%
  select(outcome_type, animal_type, intake_type, sex_upon_intake, intake_condition)

rf_control <- trainControl(method = "cv", number = 5)

set.seed(42)
rf_model <- train(
  outcome_type ~ .,
  data = rf_data,
  method = "rf",
  trControl = rf_control,
  importance = TRUE
)

rf_predictions <- predict(rf_model, newdata = test_set)

# Align factor levels
combined_levels <- union(levels(rf_predictions), levels(test_set$outcome_type))
rf_predictions <- factor(rf_predictions, levels = combined_levels)
test_set$outcome_type <- factor(test_set$outcome_type, levels = combined_levels)

rf_accuracy <- mean(rf_predictions == test_set$outcome_type)
rf_confusion <- confusionMatrix(rf_predictions, test_set$outcome_type)

rf_accuracy
rf_confusion
```

# Results

## Accuracy comparison

```{r}
absolute_improvement <- rf_accuracy - baseline_accuracy
relative_improvement <- absolute_improvement / baseline_accuracy

tibble(
  Baseline = baseline_accuracy,
  RandomForest = rf_accuracy,
  AbsoluteImprovement = absolute_improvement,
  RelativeImprovement = percent(relative_improvement, accuracy = 0.1)
)
```

## Variable importance

```{r importance-plot-top10, fig.width=12, fig.height=8}
importance_df <- varImp(rf_model)$importance %>%
  rownames_to_column("Variable") %>%
  pivot_longer(-Variable, names_to = "Outcome", values_to = "Importance") %>%
  group_by(Outcome) %>%
  slice_max(order_by = Importance, n = 10)

ggplot(importance_df, aes(x = reorder_within(Variable, Importance, Outcome), y = Importance, fill = Outcome)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ Outcome, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(
    title = "Top 10 Important Variables per Outcome Type",
    x = "Variable",
    y = "Importance"
  ) +
  theme_minimal()
```

## Model performance visualization

```{r}
plot(rf_model)
```

# Conclusion

This project explored how to predict animal shelter outcomes using features available at the time of intake.  
Two models were compared:

- **Baseline (majority class):** `r round(baseline_accuracy, 4)`  
- **Random Forest (5-fold CV):** `r round(rf_accuracy, 4)`  
- **Relative improvement:** `r percent(relative_improvement, accuracy = 0.1)`

The Random Forest model significantly outperformed the baseline,  
with a relative accuracy improvement of over 37%.  
Top predictors were `intake_type`, `sex_upon_intake`, and `intake_condition`.

# Interpretation

- Stray animals tend to be adopted more often  
- Owner-surrendered animals show different outcome trends  
- Health-related intake conditions are linked to higher euthanasia or transfer rates  
- Neutered/spayed animals may be more adoptable

# Limitations

- Strong class imbalance affects minority classes  
- No temporal or behavioral history used  
- Model not tuned for hyperparameters beyond `mtry`

# References

- Austin Animal Center Dataset: https://www.kaggle.com/datasets/aaronschlegel/austin-animal-center-shelter-intakes-and-outcomes  
- caret R package: https://topepo.github.io/caret/  
- randomForest package: https://cran.r-project.org/web/packages/randomForest  
- OpenAI (ChatGPT): Supported structure and phrasing; modeling and decisions by Yvonne Kirschler

# Appendix

```{r sessioninfo, echo=FALSE}
sessionInfo()
```